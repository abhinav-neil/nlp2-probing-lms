{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Probing Language Models for Structure"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Imports <a id=\"imports\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pickle\n",
    "from tqdm import tqdm\n",
    "import os, random\n",
    "import gdown\n",
    "from collections import defaultdict\n",
    "from lstm.model import RNNModel\n",
    "from typing import List, Dict, Tuple, Optional\n",
    "from conllu import parse_incr, TokenList\n",
    "from transformers import GPT2Tokenizer, GPT2LMHeadModel\n",
    "from ete3 import Tree as EteTree\n",
    "from scipy.sparse.csgraph import minimum_spanning_tree\n",
    "\n",
    "# torch\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Language models <a id=\"models\"></a>\n",
    "\n",
    "### Transformer\n",
    "We will use the `transformers` library of Huggingface: https://github.com/huggingface/transformers\n",
    "\n",
    "### LSTM\n",
    "We will use the Gulordava LSTM from the Colorless Green RNNs paper: https://arxiv.org/pdf/1803.11138.pdf. The weigths are available at https://drive.google.com/file/d/19Lp3AM4NEPycp_IBgoHfLc_V456pmUom/view?usp=sharing. The original code is available at https://github.com/facebookresearch/colorlessgreenRNNs/blob/master/src/language_models/model.py. The code has been altered to only output the hidden states that we are interested in. For further experiments, have a look at the original code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load transformer model and tokenizer\n",
    "tokenizer = GPT2Tokenizer.from_pretrained('distilgpt2')\n",
    "gpt_model = GPT2LMHeadModel.from_pretrained('distilgpt2')\n",
    "\n",
    "# load LSTM model and vocab\n",
    "# run this to download the saved lstm model\n",
    "lstm_path = 'lstm/state_dict.pt'  # path to saved lstm model\n",
    "if not os.path.exists(lstm_path):\n",
    "    lstm_model_url = 'https://drive.google.com/u/0/uc?id=19Lp3AM4NEPycp_IBgoHfLc_V456pmUom'\n",
    "    gdown.download(lstm_model_url, lstm_path, quiet=False)\n",
    "    \n",
    "lstm_model = RNNModel('LSTM', 50001, 650, 650, 2)\n",
    "lstm_model.load_state_dict(torch.load(lstm_path))\n",
    "\n",
    "# the LSTM uses a vocab dict that maps a token to an id, instead of a tokenizer\n",
    "with open('lstm/vocab.txt') as f:\n",
    "    w2i = {w.strip(): i for i, w in enumerate(f)}\n",
    "\n",
    "vocab = defaultdict(lambda: w2i[\"<unk>\"])\n",
    "vocab.update(w2i)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is a good idea that before you move on, you try to feed some text to your LMs; and check if everything works accordingly. "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. PoS probing <a id=\"pos probe\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "tags": [
     "globals"
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LM: lstm | using sample: False | save dir: data/lstm | model dir: models/lstm/\n"
     ]
    }
   ],
   "source": [
    "# set global variables\n",
    "lm = lstm_model  # language model, either `gpt_model` or `lstm_model`\n",
    "use_sample = False   # use a small sample of the data for faster debugging\n",
    "lm_name = 'gpt' if lm==gpt_model else 'lstm' if lm==lstm_model else None\n",
    "save_dir = f'data/sample/{lm_name}' if use_sample else f'data/{lm_name}'  # path to data\n",
    "os.makedirs(save_dir, exist_ok=True)\n",
    "model_dir = f'models/sample/{lm_name}/' if use_sample else f'models/{lm_name}/'  # path to models\n",
    "os.makedirs(model_dir, exist_ok=True)\n",
    "\n",
    "print(f'LM: {lm_name} | using sample: {use_sample} | save dir: {save_dir} | model dir: {model_dir}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1 Generate data for PoS probe <a id=\"pos data\"></a>\n",
    "We will use a treebank corpus for our data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read data\n",
    "def parse_corpus(filename: str) -> List[TokenList]:\n",
    "    data_file = open(filename, encoding=\"utf-8\")\n",
    "    ud_parses = list(parse_incr(data_file))\n",
    "    \n",
    "    return ud_parses\n",
    "\n",
    "ud_parses_sample = parse_corpus('data/sample/en_ewt-ud-train.conllu')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Generating Representations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Transformer models make use of Byte-Pair Encodings (BPE), that chunk up a piece of text in subword pieces. For example, a word such as \"largely\" could be chunked up into \"large\" and \"ly\". We are interested in probing linguistic information on the __word__-level. Therefore, we will follow the suggestion of Hewitt et al. (2019a, footnote 4), and create the representation of a word by averaging over the representations of its subwords. So the representation of \"largely\" becomes the average of that of \"large\" and \"ly\".\n",
    "\n",
    "\n",
    "2. Subword chunks never overlap multiple tokens. In other words, say we have a phrase like \"None of the\", then the tokenizer might chunk that into \"No\"+\"ne\"+\" of\"+\" the\", but __not__ into \"No\"+\"ne o\"+\"f the\", as those chunks overlap multiple tokens. \n",
    "\n",
    "\n",
    "3. Some tokens are split up into multiple pieces, that each have their own POS-tag. For example, in the first sentence the word \"Al-Zaman\" is split into \"Al\", \"-\", and \"Zaman\". In such cases, the conllu `TokenList` format will add the following attribute: `('misc', OrderedDict([('SpaceAfter', 'No')]))` to these tokens. Your model's tokenizer does not need to adhere to the same tokenization. E.g., \"Al-Zaman\" could be split into \"Al-\"+\"Za\"+\"man\", making it hard to match the representations with their correct pos-tag. Therefore I recommend you to not tokenize your entire sentence at once, but to do this based on the chunking of the treebank. <br /><br />\n",
    "Make sure to still incoporate the spaces in a sentence though, as these are part of the BPE of the tokenizer. That is, the tokenizer uses a different token id for `\"man\"`, than it does for `\" man\"`: the former could be part of `\" woman\"`=`\" wo`\"+`\"man\"`, whereas the latter would be the used in case *man* occurs at the start of a word. The tokenizer for GPT-2 adds spaces at the start of a token (represented as a `Ġ` symbol). This means that we should keep track whether the previous token had the `SpaceAfter` attribute set to `'No'`: in case it did not, we should manually prepend a `\" \"` ahead of the token.\n",
    "\n",
    "\n",
    "4. The LSTM LM does not have the issues related to subwords, but is far more restricted in its vocabulary. \n",
    "\n",
    "\n",
    "5. The huggingface transformer models don't return the hidden state by default. To achieve this you can pass `output_hidden_states=True` to a model forward pass. The hidden states are then returned for all intermediate layers as well, the latest entry in this list corresponds to the top layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00, 32.50it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00, 222.69it/s]\n"
     ]
    }
   ],
   "source": [
    "# fetch sentence representations\n",
    "def fetch_sen_reps(ud_parses: List[TokenList], model, concat=True) -> torch.Tensor:\n",
    "    '''\n",
    "    returns sentence representations (embeddings) for a list of sentences, by first tokenizing them and then passing them through the model\n",
    "    inputs:\n",
    "        ud_parses: list of sentences, each sentence is a list of tokens, each token is a dictionary (conllu format)\n",
    "        model: the model (encoder) to use for sentence representation, either an LSTM or a transformer (GPT2)\n",
    "        tokenizer: either the GPT2 tokenizer or the LSTM vocab\n",
    "        rep_size: the size of the sentence representations (embeddings)\n",
    "    returns:\n",
    "        sent_reps: a tensor of shape (num_tokens_in_corpus, representation_size), containing the sentence representations (embeddings) for all sentences in the corpus\n",
    "    '''\n",
    "    model.eval()    # set model to evaluation mode\n",
    "    sent_reps = []\n",
    "    for sent in tqdm(ud_parses):\n",
    "        # LSTM\n",
    "        if isinstance(model, RNNModel):\n",
    "            # tokenize\n",
    "            sent_tokenized = torch.tensor([vocab[token['form']] for token in sent if token[\"upostag\"] != \"_\"])\n",
    "            # get sentence representation\n",
    "            with torch.no_grad():\n",
    "                out_rep = model(sent_tokenized.unsqueeze(0), model.init_hidden(1)).squeeze(0)\n",
    "        # GPT\n",
    "        elif isinstance(model, GPT2LMHeadModel):\n",
    "            token_ids, att_masks = [], []\n",
    "            add_space = False   # whether to add a space before the token\n",
    "            for token in sent:\n",
    "                if token[\"upostag\"] == \"_\": # skip invalid/multiword tokens\n",
    "                    continue\n",
    "                # tokenize\n",
    "                tokenized = tokenizer(\" \" + token['form'], return_tensors='pt') if add_space else tokenizer(token['form'], return_tensors='pt')\n",
    "                token_ids.append(tokenized['input_ids'][0])\n",
    "                att_masks.append(tokenized['attention_mask'][0])\n",
    "                # check whether to add a space before the next token\n",
    "                add_space = False if token['misc'] is not None and token['misc'].get('SpaceAfter', '') == 'No' else True\n",
    "                \n",
    "            # get sentence representation\n",
    "            with torch.no_grad():\n",
    "                out = model(input_ids=torch.hstack(token_ids), attention_mask=torch.hstack(att_masks), output_hidden_states=True).hidden_states[-1]\n",
    "\n",
    "            # average over parts belonging to the same token\n",
    "            out_rep = torch.zeros(len(token_ids), out.shape[-1])\n",
    "            num_sub_tokens = 0\n",
    "            for i in range(out_rep.shape[0]):\n",
    "                out_rep[i] = out[i + num_sub_tokens: i + num_sub_tokens + len(token_ids[i])].mean(0)\n",
    "                num_sub_tokens += len(token_ids[i]) - 1\n",
    "                \n",
    "        else :\n",
    "            raise ValueError('model should be either an LSTM or a transformer (GPT2)')       \n",
    "        sent_reps += out_rep if concat else [out_rep]\n",
    "    \n",
    "    # stack token representations of entire corpus\n",
    "    if concat:\n",
    "        sent_reps = torch.vstack(sent_reps)\n",
    "    \n",
    "    return sent_reps\n",
    "\n",
    "# test fetch_sen_reps\n",
    "def error_msg(model_name, gold_embs, embs, i2w):\n",
    "    with open(f'{model_name}_tokens1.pickle', 'rb') as f:\n",
    "        sen_tokens = pickle.load(f)\n",
    "        \n",
    "    diff = torch.abs(embs - gold_embs)\n",
    "    max_diff = torch.max(diff)\n",
    "    avg_diff = torch.mean(diff)\n",
    "    \n",
    "    print(f\"{model_name} embeddings don't match!\")\n",
    "    print(f\"Max diff.: {max_diff:.4f}\\nMean diff. {avg_diff:.4f}\")\n",
    "\n",
    "    print(\"\\nCheck if your tokenization matches with the original tokenization:\")\n",
    "    for idx in sen_tokens.squeeze():\n",
    "        if isinstance(i2w, list):\n",
    "            token = i2w[idx]\n",
    "        else:\n",
    "            token = i2w.convert_ids_to_tokens(idx.item())\n",
    "        print(f\"{idx:<6} {token}\")\n",
    "\n",
    "\n",
    "def assert_sen_reps(model, tokenizer, lstm, vocab):\n",
    "    with open('distilgpt2_emb1.pickle', 'rb') as f:\n",
    "        distilgpt2_emb1 = pickle.load(f)\n",
    "        \n",
    "    with open('lstm_emb1.pickle', 'rb') as f:\n",
    "        lstm_emb1 = pickle.load(f)\n",
    "    \n",
    "    corpus = parse_corpus('data/sample/en_ewt-ud-train.conllu')[:1]\n",
    "    \n",
    "    own_distilgpt2_emb1 = fetch_sen_reps(corpus, model, tokenizer)\n",
    "    own_lstm_emb1 = fetch_sen_reps(corpus, lstm, vocab)\n",
    "    \n",
    "    assert distilgpt2_emb1.shape == own_distilgpt2_emb1.shape, \\\n",
    "        f\"Distilgpt2 shape mismatch: {distilgpt2_emb1.shape} (gold) vs. {own_distilgpt2_emb1.shape} (yours)\"\n",
    "    assert lstm_emb1.shape == own_lstm_emb1.shape, \\\n",
    "        f\"LSTM shape mismatch: {lstm_emb1.shape} (gold) vs. {own_lstm_emb1.shape} (yours)\"\n",
    "\n",
    "    if not torch.allclose(distilgpt2_emb1, own_distilgpt2_emb1, rtol=1e-3, atol=1e-3):\n",
    "        error_msg(\"distilgpt2\", distilgpt2_emb1, own_distilgpt2_emb1, tokenizer)\n",
    "    if not torch.allclose(lstm_emb1, own_lstm_emb1, rtol=1e-3, atol=1e-3):\n",
    "        error_msg(\"lstm\", lstm_emb1, own_lstm_emb1, list(vocab.keys()))\n",
    "\n",
    "\n",
    "assert_sen_reps(gpt_model, tokenizer, lstm_model, vocab)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Extracting PoS labels\n",
    "Next, we should define a function that extracts the corresponding POS labels for each activation. These labels will be transformed to a tensor containing the label index for each item."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fetch POS tags\n",
    "def fetch_pos_tags(ud_parses: List[TokenList], pos_vocab: Optional[Dict[str, int]] = None) -> Tuple[torch.Tensor, Dict[str, int]]:\n",
    "    '''\n",
    "    return the POS tags for all tokens in the corpus\n",
    "    inputs:\n",
    "        ud_parses: list of sentences, each sentence is a list of tokens, each token is a dictionary (conllu format)\n",
    "        pos_vocab: a dictionary mapping POS tags to integers (optional)\n",
    "    returns:\n",
    "        pos_tags: a tensor of shape (num_tokens_in_corpus,) containing the POS tags for all tokens in the corpus\n",
    "    '''\n",
    "    if pos_vocab is None:\n",
    "        pos_vocab = defaultdict(int)\n",
    "        for sent in ud_parses:\n",
    "            for token in sent:\n",
    "                # add new POS tags to vocab \n",
    "                if token[\"upostag\"] not in pos_vocab and token[\"upostag\"] != \"_\":\n",
    "                    pos_vocab[token[\"upostag\"]] = len(pos_vocab)\n",
    "\n",
    "    pos_tags = [torch.tensor(pos_vocab[token[\"upostag\"]]) for sent in ud_parses for token in sent if token[\"upostag\"] != \"_\"]\n",
    "    pos_tags = torch.vstack(pos_tags).squeeze()\n",
    "\n",
    "    return pos_tags, pos_vocab\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Merge representations & PoS tags\n",
    "We merge sentence representations (features) and PoS tags (labels) to create dataloaders for the probe. We pass the `train_vocab` to the data creation of the `dev` and `test` data is that we want to use the same label vocabulary across the different train/dev/test splits."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "size of train data: 204585 | size of dev data: 25148 | size of test data: 25096\n",
      "CPU times: user 3.12 ms, sys: 92 ms, total: 95.1 ms\n",
      "Wall time: 94.1 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# create 2 tensors for a .conllu file: 1 containing the token representations, and 1 containing the (tokenized) pos_tags\n",
    "def create_data(filename: str, lm, pos_vocab=None):\n",
    "    print('parsing corpus...')\n",
    "    ud_parses = parse_corpus(filename)\n",
    "    print('fetching sentence representations...')\n",
    "    sen_reps = fetch_sen_reps(ud_parses, lm)\n",
    "    print('fetching POS tags...')\n",
    "    pos_tags, pos_vocab = fetch_pos_tags(ud_parses, pos_vocab=pos_vocab)    \n",
    "    return sen_reps, pos_tags, pos_vocab\n",
    "\n",
    "# create datasets and dataloaders\n",
    "# define a custom PyTorch dataset\n",
    "class MyDataset(Dataset):\n",
    "    def __init__(self, x, y):\n",
    "        self.x = x\n",
    "        self.y = y\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        return self.x[index], self.y[index]\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.x)\n",
    "\n",
    "# create train/dev/test data\n",
    "# path to the .conllu files\n",
    "train_path = 'data/sample/en_ewt-ud-train.conllu' if use_sample else 'data/en_ewt-ud-train.conllu'\n",
    "dev_path = 'data/sample/en_ewt-ud-dev.conllu' if use_sample else 'data/en_ewt-ud-dev.conllu'\n",
    "test_path = 'data/sample/en_ewt-ud-test.conllu' if use_sample else 'data/en_ewt-ud-test.conllu'\n",
    "\n",
    "# create or load the data\n",
    "try:\n",
    "    train_x_pos, train_y_pos, train_vocab_pos = torch.load(f'{save_dir}/train_x_pos.pt'), torch.load(f'{save_dir}/train_y_pos.pt'), torch.load(f'{save_dir}/train_vocab_pos.pt')\n",
    "except FileNotFoundError:\n",
    "    print('creating train data...')\n",
    "    train_x_pos, train_y_pos, train_vocab_pos = create_data(train_path, lm)\n",
    "    torch.save(train_x_pos, f'{save_dir}/train_x_pos.pt')\n",
    "    torch.save(train_y_pos, f'{save_dir}/train_y_pos.pt')\n",
    "    torch.save(train_vocab_pos, f'{save_dir}/train_vocab_pos.pt')\n",
    "try:\n",
    "    dev_x_pos, dev_y_pos = torch.load(f'{save_dir}/dev_x_pos.pt'), torch.load(f'{save_dir}/dev_y_pos.pt')\n",
    "except FileNotFoundError:\n",
    "    print('creating dev data...')\n",
    "    dev_x_pos, dev_y_pos, _ = create_data(dev_path, lm, pos_vocab=train_vocab_pos)\n",
    "    torch.save(dev_x_pos, f'{save_dir}/dev_x_pos.pt')\n",
    "    torch.save(dev_y_pos, f'{save_dir}/dev_y_pos.pt')\n",
    "try:\n",
    "    test_x_pos, test_y_pos = torch.load(f'{save_dir}/test_x_pos.pt'), torch.load(f'{save_dir}/test_y_pos.pt')\n",
    "except FileNotFoundError:\n",
    "    print('creating test data...')\n",
    "    test_x_pos, test_y_pos, _ = create_data(test_path, lm, pos_vocab=train_vocab_pos)\n",
    "    torch.save(test_x_pos, f'{save_dir}/test_x_pos.pt')\n",
    "    torch.save(test_y_pos, f'{save_dir}/test_y_pos.pt')\n",
    "    \n",
    "# create dataloaders\n",
    "train_data_pos = MyDataset(train_x_pos, train_y_pos)\n",
    "dev_data_pos = MyDataset(dev_x_pos, dev_y_pos)\n",
    "test_data_pos = MyDataset(test_x_pos, test_y_pos)\n",
    "print(f'size of train data: {len(train_data_pos)} | size of dev data: {len(dev_data_pos)} | size of test data: {len(test_data_pos)}')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2 Train PoS probe <a name=\"dc\"></a>\n",
    "We will train a PoS probe using simple linear model. Refer \"Designing and Interpreting Probes with Control Tasks\" by Hewitt and Liang (esp. Sec. 3.2)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Diagnostic classifier/probe\n",
    "# class to store training parameters\n",
    "class TrainingParams:\n",
    "    def __init__(self, lr=1e-3, batch_size=256, num_epochs=1000, patience=10):\n",
    "        self.lr = lr\n",
    "        self.batch_size = batch_size\n",
    "        self.num_epochs = num_epochs\n",
    "        self.patience = patience\n",
    "        \n",
    "def set_seed(seed):\n",
    "    # Set seed for random, numpy, PyTorch\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "        \n",
    "def train_pos_probe(model, train_data, val_data, params, seed=42, print_every=10):\n",
    "    set_seed(seed)  # set seed for reproducibility\n",
    "    # create dataloaders\n",
    "    train_loader = DataLoader(train_data, batch_size=params.batch_size, shuffle=True)\n",
    "    val_loader = DataLoader(val_data, batch_size=params.batch_size, shuffle=False)\n",
    "    # define loss and optimizer\n",
    "    criterion = torch.nn.CrossEntropyLoss()\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=params.lr)\n",
    "    val_losses, val_accs = [], []\n",
    "\n",
    "    # training/val loop\n",
    "    for epoch in range(params.num_epochs):\n",
    "        # train\n",
    "        model.train()\n",
    "        for train_x, train_y in train_loader:\n",
    "            out = model(train_x)\n",
    "            loss = criterion(out, train_y)\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward(retain_graph=True)\n",
    "            optimizer.step()\n",
    "            \n",
    "        # validate\n",
    "        model.eval()\n",
    "        val_losses_epoch, val_accs_epoch = [], []\n",
    "        for val_x, val_y in val_loader:\n",
    "            with torch.no_grad():\n",
    "                out = model(val_x)\n",
    "                loss = criterion(out, val_y)\n",
    "                val_losses_epoch.append(loss.item())\n",
    "                preds_val = torch.argmax(out, dim=1)\n",
    "                acc = (preds_val == val_y).sum().item() / len(val_y)\n",
    "                val_accs_epoch.append(acc)\n",
    "                \n",
    "        val_loss_epoch = np.mean(val_losses_epoch)\n",
    "        val_acc_epoch = np.mean(val_accs_epoch)\n",
    "        val_losses.append(val_loss_epoch)\n",
    "        val_accs.append(val_acc_epoch)\n",
    "        \n",
    "        if epoch % print_every == 0:\n",
    "            print(f'epoch: {epoch} | val loss: {val_loss_epoch:.3f} | val acc: {val_acc_epoch:.3f}')\n",
    "        \n",
    "        # early stopping\n",
    "        if epoch >= params.patience and val_loss_epoch > val_losses[-params.patience]:\n",
    "            print(f'val loss did not improve for {params.patience} epochs, stopping training')\n",
    "            break\n",
    "        \n",
    "    # save model\n",
    "    # model_path = f'{model_dir}/linear_pos_probe.pt'\n",
    "    # torch.save(model, model_path)\n",
    "        \n",
    "    return model, val_losses, val_accs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training nonlinear pos probe with lstm embeddings...\n",
      "epoch: 0 | val loss: 0.373 | val acc: 0.884\n",
      "epoch: 10 | val loss: 0.317 | val acc: 0.901\n",
      "val loss did not improve for 10 epochs, stopping training\n",
      "test accuracy of nonlinear pos probe  using lstm embeddings is 0.896\n"
     ]
    }
   ],
   "source": [
    "# train pos probe\n",
    "pos_probe_type = 'nonlinear'    # 'linear' or 'nonlinear'\n",
    "try:\n",
    "    pos_probe_model = torch.load(f'{model_dir}/{pos_probe_type}_pos_probe.pt')\n",
    "except FileNotFoundError:\n",
    "    params = TrainingParams()\n",
    "    if pos_probe_type == 'linear':\n",
    "        # single linear layer with input_dim = embedding_dim and output_dim = len(pos_vocab), no activation\n",
    "        pos_probe_model = nn.Linear(train_x_pos.shape[1], len(train_vocab_pos))\n",
    "    elif pos_probe_type == 'nonlinear':\n",
    "        # two linear layers of shape (embedding_dim, hidden_dim) and (hidden_dim, len(pos_vocab)), with ReLU activation in between\n",
    "        hidden_dim = 100\n",
    "        pos_probe_model = nn.Sequential(\n",
    "            nn.Linear(train_x_pos.shape[1], hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_dim, len(train_vocab_pos)))\n",
    "    else:\n",
    "        raise ValueError('pos_probe_type must be linear or nonlinear')\n",
    "    \n",
    "    print(f'training {pos_probe_type} pos probe with {lm_name} embeddings...')\n",
    "    pos_probe_model, _, _ = train_pos_probe(pos_probe_model, train_data_pos, dev_data_pos, params)\n",
    "    torch.save(pos_probe_model, f'{model_dir}/{pos_probe_type}_pos_probe.pt')\n",
    "\n",
    "# test\n",
    "out_test = pos_probe_model(test_x_pos)\n",
    "preds_test_pos = torch.argmax(out_test, dim=1)\n",
    "test_acc_pos = (preds_test_pos == test_y_pos).sum().item() / len(test_y_pos)\n",
    "print(f'test accuracy of {pos_probe_type} pos probe  using {lm_name} embeddings is {test_acc_pos:.3f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "avg PoS accuracy of 1st 5 sentences in test set using gpt embeddings: [0.8571428571428571, 0.9130434782608695, 1.0, 0.96, 0.967741935483871]\n"
     ]
    }
   ],
   "source": [
    "# compute avg PoS accuracy per sentence in test set\n",
    "def get_pos_acc_sent(ud_parses, preds_pos, y_pos):\n",
    "    '''\n",
    "    Compute average PoS accuracy per sentence in test set\n",
    "    Inputs:\n",
    "        ud_parses: list of lists of UD parse trees\n",
    "        preds_pos: predicted PoS tags\n",
    "        y_pos: true PoS tags\n",
    "    Returns:\n",
    "        accs_pos_sent: list of average PoS accuracies per sentence\n",
    "    '''\n",
    "    token_count = 0\n",
    "    accs_pos_sent = []\n",
    "    for sent in test_ud_parses:\n",
    "        acc_pos_sent = (preds_pos[token_count:token_count+len(sent)] == y_pos[token_count:token_count+len(sent)]).sum().item() / len(sent)\n",
    "        accs_pos_sent.append(acc_pos_sent)\n",
    "        token_count += len(sent)\n",
    "    return accs_pos_sent\n",
    "\n",
    "test_ud_parses = parse_corpus(test_path)\n",
    "test_accs_pos_sent = get_pos_acc_sent(test_ud_parses, preds_test_pos, test_y_pos)\n",
    "print(f'avg PoS accuracy of 1st 5 sentences in test set using {lm_name} embeddings: {test_accs_pos_sent[:5]}')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Structural probing <a name=\"structural probe\"></a>"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.1 Trees <a name=\"trees\"></a>\n",
    "\n",
    "For our gold labels, we need to recover the node distances from our parse tree. For this we will use the functionality provided by `ete3`, that allows us to compute that directly. I have provided code that transforms a `TokenTree` to a `Tree` in `ete3` format."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Helper functions to tranform trees"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "# transform your conllu tree to an nltk.Tree, for better visualisation\n",
    "def rec_tokentree_to_nltk(tokentree):\n",
    "    token = tokentree.token[\"form\"]\n",
    "    tree_str = f\"({token} {' '.join(rec_tokentree_to_nltk(t) for t in tokentree.children)})\"\n",
    "    return tree_str\n",
    "\n",
    "def tokentree_to_nltk(tokentree):\n",
    "    from nltk import Tree as NLTKTree\n",
    "    tree_str = rec_tokentree_to_nltk(tokentree)\n",
    "    return NLTKTree.fromstring(tree_str)\n",
    "class FancyTree(EteTree):\n",
    "    def __init__(self, *args, **kwargs):\n",
    "        super().__init__(*args, format=1, **kwargs)\n",
    "        \n",
    "    def __str__(self):\n",
    "        return self.get_ascii(show_internal=True)\n",
    "    \n",
    "    def __repr__(self):\n",
    "        return str(self)\n",
    "    \n",
    "# transform your conllu tree to an ete3.Tree, for better visualisation\n",
    "def rec_tokentree_to_ete(tokentree):\n",
    "    idx = str(tokentree.token[\"id\"])\n",
    "    children = tokentree.children\n",
    "    if children:\n",
    "        return f\"({','.join(rec_tokentree_to_ete(t) for t in children)}){idx}\"\n",
    "    else:\n",
    "        return idx\n",
    "    \n",
    "def tokentree_to_ete(tokentree):\n",
    "    newick_str = rec_tokentree_to_ete(tokentree)\n",
    "    return FancyTree(f\"{newick_str};\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read in a corpus and convert it to an ete3 Tree\n",
    "def parse_corpus(filename):\n",
    "    data_file = open(filename, encoding=\"utf-8\")\n",
    "    ud_parses = list(parse_incr(data_file))\n",
    "    return ud_parses\n",
    "\n",
    "corpus = parse_corpus('data/sample/en_ewt-ud-train.conllu')\n",
    "# visualize the first tree\n",
    "# item = corpus[0]\n",
    "# tokentree = item.to_tree()\n",
    "# ete3_tree = tokentree_to_ete(tokentree)\n",
    "# print(ete3_tree)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Computing gold distances, MST & UUAS scores\n",
    "\n",
    "We label a token by its token id (converted to a string). Based on these id's we are going to retrieve the node distances.\n",
    "\n",
    "To create the true distances of a parse tree in our treebank, we are going to use the `.get_distance` method that is provided by `ete3`: http://etetoolkit.org/docs/latest/tutorial/tutorial_trees.html#working-with-branch-distances. We will store all these distances in a `torch.Tensor`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_gold_distances(corpus):\n",
    "    '''Create a list of gold distances for each sentence in the corpus.'''\n",
    "    all_distances = []\n",
    "\n",
    "    for item in tqdm(corpus):\n",
    "        tokentree = item.to_tree()\n",
    "        ete_tree = tokentree_to_ete(tokentree)\n",
    "\n",
    "        sen_len = len(ete_tree.search_nodes())\n",
    "        distances = torch.zeros((sen_len, sen_len))\n",
    "\n",
    "        for node1 in ete_tree.search_nodes():\n",
    "            for node2 in ete_tree.search_nodes():\n",
    "                distances[int(node1.name)-1][int(node2.name)-1] = node1.get_distance(node2)\n",
    "\n",
    "        all_distances.append(distances)\n",
    "\n",
    "    return all_distances"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The next step is now to do the previous step the other way around. After all, we are mainly interested in predicting the node distances of a sentence, in order to recreate the corresponding parse tree.\n",
    "\n",
    "Hewitt et al. reconstruct a parse tree based on a _minimum spanning tree_ (MST, https://en.wikipedia.org/wiki/Minimum_spanning_tree). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_mst(distances):\n",
    "    '''Create a minimum spanning tree from a distance matrix.'''\n",
    "    distances = torch.triu(distances).detach().numpy()\n",
    "    mst = minimum_spanning_tree(distances).toarray()\n",
    "    mst[mst>0] = 1.\n",
    "    \n",
    "    return mst"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's have a look at what this looks like, by looking at a relatively short sentence in the sample corpus. This then shows you the original parse tree, the distances between the nodes, and the MST that is retrieved from these distances. Can you spot the edges in the MST matrix that correspond to the edges in the parse tree?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "# viz ete tree, gold distances, mst\n",
    "# item = corpus[5]\n",
    "# tokentree = item.to_tree()\n",
    "# ete3_tree = tokentree_to_ete(tokentree)\n",
    "# print(ete3_tree, '\\n')\n",
    "\n",
    "# gold_distance = create_gold_distances(corpus[5:6])[0]\n",
    "# print(gold_distance, '\\n')\n",
    "\n",
    "# mst = create_mst(gold_distance)\n",
    "# print(mst)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we are able to map edge distances back to parse trees, we can create code for our quantitative evaluation. For this we will use the Undirected Unlabeled Attachment Score (UUAS), which is expressed as:\n",
    "\n",
    "$$\\frac{\\text{number of predicted edges that are an edge in the gold parse tree}}{\\text{number of edges in the gold parse tree}}$$\n",
    "\n",
    "To do this, we will need to obtain all the edges from our MST matrix. Note that, since we are using undirected trees, that an edge can be expressed in 2 ways: an edge between node $i$ and node $j$ is denoted by both `mst[i,j] = 1`, or `mst[j,i] = 1`.\n",
    "\n",
    "You will write code that computes the UUAS score for a matrix of predicted distances, and the corresponding gold distances. I recommend you to split this up into 2 methods: 1 that retrieves the edges that are present in an MST matrix, and one general method that computes the UUAS score."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[-1]"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = []\n",
    "b = 0\n",
    "a.append(b) if b > 0 else a.append(-1)\n",
    "a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_edges(mst):\n",
    "    edges = np.nonzero(mst)\n",
    "    edges = list(zip(edges[0], edges[1]))\n",
    "    edges = set(map(lambda x: tuple(sorted(x)), edges))\n",
    "    return edges\n",
    "\n",
    "\n",
    "def calc_uuas(pred_distances, gold_distances):  \n",
    "    # uuas = 0\n",
    "    uuas_batch = []\n",
    "    length = 0\n",
    "    for i in range(len(gold_distances)):\n",
    "        l = max(torch.nonzero(gold_distances[i] != -1, as_tuple=True)[0]) + 1\n",
    "        pred_mst = create_mst(pred_distances[i][:l, :l])\n",
    "        gold_mst = create_mst(gold_distances[i][:l, :l])\n",
    "        pred_edges = get_edges(pred_mst)\n",
    "        gold_edges = get_edges(gold_mst)\n",
    "        # try:\n",
    "        #     uuas += len(pred_edges.intersection(gold_edges)) / len(gold_edges)\n",
    "        #     length += 1\n",
    "        # except:\n",
    "        #     pass\n",
    "        uuas_sent = len(pred_edges.intersection(gold_edges)) / len(gold_edges) if len(gold_edges) > 0 else -1\n",
    "        uuas_batch.append(uuas_sent)\n",
    "\n",
    "    # return uuas / length\n",
    "    return uuas_batch"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2 Define structural probe class & L1 loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "# structural probe class (from John Hewitt)\n",
    "class StructuralProbe(nn.Module):\n",
    "    \"\"\" Computes squared L2 distance after projection by a matrix.\n",
    "    For a batch of sentences, computes all n^2 pairs of distances\n",
    "    for each sentence in the batch.\n",
    "    \"\"\"\n",
    "    def __init__(self, model_dim, rank, device=\"cpu\"):\n",
    "        super().__init__()\n",
    "        self.probe_rank = rank\n",
    "        self.model_dim = model_dim\n",
    "        \n",
    "        self.proj = nn.Parameter(data = torch.zeros(self.model_dim, self.probe_rank))\n",
    "        \n",
    "        nn.init.uniform_(self.proj, -0.05, 0.05)\n",
    "        self.to(device)\n",
    "\n",
    "    def forward(self, batch):\n",
    "        \"\"\" Computes all n^2 pairs of distances after projection\n",
    "        for each sentence in a batch.\n",
    "        Note that due to padding, some distances will be non-zero for pads.\n",
    "        Computes (B(h_i-h_j))^T(B(h_i-h_j)) for all i,j\n",
    "        Args:\n",
    "          batch: a batch of word representations of the shape\n",
    "            (batch_size, max_seq_len, representation_dim)\n",
    "        Returns:\n",
    "          A tensor of distances of shape (batch_size, max_seq_len, max_seq_len)\n",
    "        \"\"\"\n",
    "        transformed = torch.matmul(batch, self.proj)\n",
    "        \n",
    "        batchlen, seqlen, rank = transformed.size()\n",
    "        \n",
    "        transformed = transformed.unsqueeze(2)\n",
    "        transformed = transformed.expand(-1, -1, seqlen, -1)\n",
    "        transposed = transformed.transpose(1,2)\n",
    "        \n",
    "        diffs = transformed - transposed\n",
    "        \n",
    "        squared_diffs = diffs.pow(2)\n",
    "        squared_distances = torch.sum(squared_diffs, -1)\n",
    "\n",
    "        return squared_distances\n",
    "\n",
    "    \n",
    "class L1DistanceLoss(nn.Module):\n",
    "    \"\"\"Custom L1 loss for distance matrices.\"\"\"\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "    def forward(self, predictions, label_batch, length_batch):\n",
    "        \"\"\" Computes L1 loss on distance matrices.\n",
    "        Ignores all entries where label_batch=-1\n",
    "        Normalizes first within sentences (by dividing by the square of the sentence length)\n",
    "        and then across the batch.\n",
    "        Args:\n",
    "          predictions: A pytorch batch of predicted distances\n",
    "          label_batch: A pytorch batch of true distances\n",
    "          length_batch: A pytorch batch of sentence lengths\n",
    "        Returns:\n",
    "          A tuple of:\n",
    "            batch_loss: average loss in the batch\n",
    "            total_sents: number of sentences in the batch\n",
    "        \"\"\"\n",
    "        labels_1s = (label_batch != -1).float()\n",
    "        predictions_masked = predictions * labels_1s\n",
    "        labels_masked = label_batch * labels_1s\n",
    "        total_sents = torch.sum((length_batch != 0)).float()\n",
    "        squared_lengths = length_batch.pow(2).float()\n",
    "\n",
    "        if total_sents > 0:\n",
    "            loss_per_sent = torch.sum(torch.abs(predictions_masked - labels_masked), dim=(1,2))\n",
    "            normalized_loss_per_sent = loss_per_sent / squared_lengths\n",
    "            batch_loss = torch.sum(normalized_loss_per_sent) / total_sents\n",
    "        \n",
    "        else:\n",
    "            batch_loss = torch.tensor(0.0)\n",
    "        \n",
    "        return batch_loss, total_sents\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.3 Create data for structural probes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "size of train set: 12543 | size of dev set: 2002 | size of test set: 2077\n"
     ]
    }
   ],
   "source": [
    "def init_corpus(path, model, concat=False, cutoff=None):\n",
    "    \"\"\" Initialises the data of a corpus.\n",
    "    \n",
    "    Inputs:\n",
    "        path : str\n",
    "            Path to corpus location\n",
    "        model: language model to encode sentences, either LSTM or GPT2\n",
    "        concat : bool, optional\n",
    "            Optional toggle to concatenate all the tensors\n",
    "            returned by `fetch_sen_reps`.\n",
    "        cutoff : int, optional\n",
    "            Optional integer to \"cutoff\" the data in the corpus.\n",
    "            This allows only a subset to be used, alleviating \n",
    "            memory usage.\n",
    "    Returns:\n",
    "        embs : torch.Tensor \n",
    "            embeddings tensor of shape (num_tokens_in_corpus, model_dim)\n",
    "        gold_distances : torch.Tensor \n",
    "            gold distances tensor of shape (num_sentences_in_corpus, max_sentence_length, max_sentence_length)\n",
    "    \"\"\"\n",
    "    print('parsing corpus...')\n",
    "    corpus = parse_corpus(path)[:cutoff]\n",
    "    print('fetching sentence representations...')\n",
    "    embs = fetch_sen_reps(corpus, model, concat=concat)    \n",
    "    print('computing gold distances...')\n",
    "    gold_distances = create_gold_distances(corpus)\n",
    "    \n",
    "    return embs, gold_distances\n",
    "\n",
    "# create data for structural probe\n",
    "try:\n",
    "    train_data_str = torch.load(f'{save_dir}/train_data_str.pt')\n",
    "    val_data_str = torch.load(f'{save_dir}/val_data_str.pt')\n",
    "    test_data_str = torch.load(f'{save_dir}/test_data_str.pt')\n",
    "except FileNotFoundError:\n",
    "    print('generating data for structural probe...')\n",
    "    print('train')\n",
    "    train_x_str, train_y_str = init_corpus(train_path, lm)\n",
    "    train_data_str = MyDataset(train_x_str, train_y_str)\n",
    "    print('dev')\n",
    "    val_x_str, val_y_str = init_corpus(dev_path, lm)\n",
    "    val_data_str = MyDataset(val_x_str, val_y_str)\n",
    "    print('test')\n",
    "    test_x_str, test_y_str = init_corpus(test_path, lm)\n",
    "    test_data_str = MyDataset(test_x_str, test_y_str)\n",
    "    torch.save(train_data_str, f'{save_dir}/train_data_str.pt')\n",
    "    torch.save(val_data_str, f'{save_dir}/val_data_str.pt')\n",
    "    torch.save(test_data_str, f'{save_dir}/test_data_str.pt')\n",
    "\n",
    "print(f'size of train set: {len(train_data_str)} | size of dev set: {len(val_data_str)} | size of test set: {len(test_data_str)}')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.4 Train structural probe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "# evaluate structural probe\n",
    "def evaluate_probe(model, dataloader, loss_fn):\n",
    "    # global TREE\n",
    "    model.eval()\n",
    "    loss = 0\n",
    "    # uuas = 0\n",
    "    uuas = []\n",
    "    # spearman = 0\n",
    "    with torch.no_grad():\n",
    "      for x, gold_distances, length in dataloader:\n",
    "          preds = model(x)\n",
    "          loss += loss_fn(preds, gold_distances, length)[0]\n",
    "          uuas += calc_uuas(preds, gold_distances)\n",
    "        #   spearman += calc_spearman(preds, gold_distances)\n",
    "          # Code to create a latex tree\n",
    "        #   if i == 13 and TREE == False and test == True:\n",
    "        #     item = 13\n",
    "        #     words = get_words(f'data/{path}-test.conllu', item)\n",
    "\n",
    "        #     l = max(torch.nonzero(gold_distances[i - 1]!=-1, as_tuple=True)[0]) +1\n",
    "        #     pred_mst = create_mst(preds[i - 1][:l,:l])\n",
    "        #     gold_mst = create_mst(gold_distances[i - 1][:l,:l])\n",
    "        #     pred_edges = edges(pred_mst)\n",
    "        #     gold_edges = edges(gold_mst)\n",
    "        #     print_tikz(pred_edges, gold_edges, words, model_type)\n",
    "        #     TREE = True\n",
    "    loss /= len(dataloader)\n",
    "    # uuas /= len(dataloader)\n",
    "    # take mean of uuas across batches where uuas != -1\n",
    "    uuas_avg = sum([x for x in uuas if x != -1])/len([x for x in uuas if x != -1])\n",
    "    # spearman /= len(dataloader)\n",
    "\n",
    "    return loss, uuas_avg, uuas\n",
    "\n",
    "def pad_collate_fn(batch):\n",
    "    max_length = max([len(x[1]) for x in batch])\n",
    "    out_labels = torch.full((len(batch), max_length, max_length), -1)\n",
    "    out_lengths = torch.zeros(len(batch))\n",
    "    for i, x in enumerate(batch):\n",
    "      out_labels[i, :x[1].shape[0], :x[1].shape[1]] = x[1]\n",
    "      out_lengths[i] = x[1].shape[0]\n",
    "      if len(x[0].shape) == 1:\n",
    "        batch[i] = (x[0].unsqueeze(0), x[1])\n",
    "    return torch.nn.utils.rnn.pad_sequence(list(map(lambda x: x[0].detach(), batch)), batch_first = True, padding_value=-1), out_labels, out_lengths\n",
    "\n",
    "def train_structural_probe(model, train_data, val_data, params, seed=42, print_every=10):\n",
    "    # create dataloaders\n",
    "    set_seed(seed)  # set seed for reproducibility\n",
    "    train_loader = DataLoader(train_data, batch_size=params.batch_size, shuffle=True, collate_fn=pad_collate_fn)\n",
    "    val_loader = DataLoader(val_data, batch_size=params.batch_size, shuffle=False, collate_fn=pad_collate_fn)\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=params.lr)\n",
    "    scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', factor=0.5, patience=1)\n",
    "    criterion =  L1DistanceLoss()\n",
    "    val_losses, val_uuas = [], []\n",
    "\n",
    "    # training/val loop\n",
    "    print(f'training structural probe with {lm_name} embeddings...')\n",
    "    for epoch in range(params.num_epochs):\n",
    "        # train\n",
    "        model.train()\n",
    "        for train_x, gold_distances, lengths in train_loader:\n",
    "            out = model(train_x)\n",
    "            loss = criterion(out, gold_distances, lengths)[0]\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward(retain_graph=True)\n",
    "            optimizer.step()\n",
    "            \n",
    "        # val\n",
    "        val_loss_epoch, val_uuas_epoch, _ = evaluate_probe(model, val_loader, criterion)\n",
    "        # Using a scheduler is up to you, and might require some hyper param fine-tuning\n",
    "        scheduler.step(val_loss_epoch)\n",
    "        val_losses.append(val_loss_epoch)\n",
    "        val_uuas.append(val_uuas_epoch)\n",
    "        \n",
    "        if epoch % print_every == 0:\n",
    "            print(f'epoch: {epoch} | val loss: {val_loss_epoch:.3f} | val uuas: {val_uuas_epoch:.3f}')\n",
    "        \n",
    "        # early stopping\n",
    "        if epoch >= params.patience and val_loss_epoch > val_losses[-params.patience]:\n",
    "            print(f'val loss did not improve for {params.patience} epochs, stopping training')\n",
    "            break\n",
    "        \n",
    "    # save model\n",
    "    # model_path = f'{model_dir}/str_probe.pt'\n",
    "    # torch.save(model, model_path)\n",
    "        \n",
    "    return model, val_losses, val_uuas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loaded saved structural probe model using gpt embeddings\n",
      "test uuas of structural pos probe using gpt embeddings: 0.689, test loss: 0.538\n",
      "CPU times: user 42 s, sys: 394 ms, total: 42.4 s\n",
      "Wall time: 2.7 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# train structural probe\n",
    "params = TrainingParams()\n",
    "try:\n",
    "    str_probe_model = torch.load(f'{model_dir}/str_probe.pt')\n",
    "    print(f'loaded saved structural probe model using {lm_name} embeddings')\n",
    "except FileNotFoundError:\n",
    "    str_probe_model = StructuralProbe(train_data_str[0][0].shape[1], rank=64)\n",
    "    str_probe_model, _, _ = train_structural_probe(str_probe_model, train_data_str, val_data_str, params)\n",
    "    torch.save(str_probe_model, f'{model_dir}/str_probe.pt')\n",
    "\n",
    "# test\n",
    "test_loader_str = DataLoader(test_data_str, batch_size=params.batch_size, shuffle=False, collate_fn=pad_collate_fn)\n",
    "test_loss_str, test_uuas_str_avg, test_uuas_str   = evaluate_probe(str_probe_model, test_loader_str, L1DistanceLoss())\n",
    "print(f'test uuas of structural pos probe using {lm_name} embeddings: {test_uuas_str_avg:.3f}, test loss: {test_loss_str:.3f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "correlation b/w PoS accuracy and uuas of structural probe per sentence on test set using gpt embeddings: 0.057\n"
     ]
    }
   ],
   "source": [
    "# find correlation b/w PoS accuracy and uuas of structural probe on test set\n",
    "from scipy.stats import spearmanr\n",
    "corr_pos_acc_uuas_test = spearmanr([test_accs_pos_sent[i] for i in range(len(test_accs_pos_sent)) if test_uuas_str[i] != -1], [x for x in test_uuas_str if x != -1])[0]\n",
    "print(f'correlation b/w PoS accuracy and uuas of structural probe per sentence on test set using {lm_name} embeddings: {corr_pos_acc_uuas_test:.3f}')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Print trees to LaTeX\n",
    "Code to print dependency tree plots in LaTeX like those of Figure 2 in the Structural Probing paper. \n",
    "**N.B.**: for the latex tikz tree the first token in a sentence has index 1 (instead of 0), so take that into account with the predicted and gold edges that you pass to the method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_tikz(predicted_edges, gold_edges, words):\n",
    "    \"\"\" Turns edge sets on word (nodes) into tikz dependency LaTeX.\n",
    "    Parameters\n",
    "    ----------\n",
    "    predicted_edges : Set[Tuple[int, int]]\n",
    "        Set (or list) of edge tuples, as predicted by your probe.\n",
    "    gold_edges : Set[Tuple[int, int]]\n",
    "        Set (or list) of gold edge tuples, as obtained from the treebank.\n",
    "    words : List[str]\n",
    "        List of strings representing the tokens in the sentence.\n",
    "    \"\"\"\n",
    "\n",
    "    string = \"\"\"\\\\begin{dependency}[hide label, edge unit distance=.5ex]\n",
    "    \\\\begin{deptext}[column sep=0.05cm]\n",
    "    \"\"\"\n",
    "\n",
    "    string += (\n",
    "        \"\\\\& \".join([x.replace(\"$\", \"\\$\").replace(\"&\", \"+\") for x in words])\n",
    "        + \" \\\\\\\\\\n\"\n",
    "    )\n",
    "    string += \"\\\\end{deptext}\" + \"\\n\"\n",
    "    for i_index, j_index in gold_edges:\n",
    "        string += \"\\\\depedge[-]{{{}}}{{{}}}{{{}}}\\n\".format(i_index, j_index, \".\")\n",
    "    for i_index, j_index in predicted_edges:\n",
    "        string += f\"\\\\depedge[-,edge style={{red!60!}}, edge below]{{{i_index}}}{{{j_index}}}{{.}}\\n\"\n",
    "    string += \"\\\\end{dependency}\\n\"\n",
    "    print(string)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.2"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
